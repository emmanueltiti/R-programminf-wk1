---
title: "week_1 R_programming"
author: "Emmanuel Titi"
date: "30 May 2022"
output: html_document
---

*A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.*

**RESEARCH QUESTION**

What characterristics does the person clicking the link posses?

*Metric for success*

Coming up with a list that generalizes in characteristics or patterns that all ad clickers have would form a good foundation in making educated guess or almost precise predictions on other people with similar traits online.

*Context*

This research would be mostly appropriate in the client is looking to make more targeted ads such that the ads go to a precise group on individuals who fit the criteria we come up with in the at the end.

**BASIC DATA ANALYSIS**

Here we try to get familiar with our data set ,its shape ,sum of unique values on each columns and much more.This sort of gives us ideas on how to approach our problem solving works.

```{r}
#importing needed dependencies for analysis and cleaning 
library(tidyr)
library(tidyverse)
#pre veiwing
df_ads=read.csv("http://bit.ly/IPAdvertisingData")
head(df_ads ,6)
tail(df_ads ,6)
```

We start off by first seeing the shape of our data set ie. the number of columns and rows.

```{r}
dim(df_ads)
```

Getting to know the data types of our variables is essential ,helps in knowing how to compare their relationship and ultimately being very useful in our plots

```{r}
#checking for data types of variables
sapply(df_ads,class)
```

The data type seem to be on point. Lets try and get a summary of our data set

```{r}
summary(df_ads)
```

The summary sort of gives us the basic information we need to understand the scope of every variable we have. For instance we can see the that for the Age variable the minimum age of an individual in our data frame is 19 and maximum is 61 . we have a mean of 36 years fo that .

**DATA CLEANING**

***Checking for null values***

Null values make our data inconsistent and may make our analysis hard,for that it is necessary to properly deal with them to improve the quality of the data we have.

```{r}
#checking for the count of missing values
sapply(df_ads, function(x) sum(is.na(x)))
sum(is.na(df_ads))
```

It appears our data frame has no missing values.

***Checking for duplicates***

```{r}
sum(duplicated(df_ads))
```

There are also zero duplicates

***Checking for outliers*** **Box**

plots are a great way of visualizing outliers

```{r}
boxplot(df_ads$Daily.Time.Spent.on.Site)
boxplot(df_ads$Age)
boxplot(df_ads$Area.Income)
boxplot(df_ads$Daily.Internet.Usage)
```

The area income appears to have outliers ,basically values below the 20000 mark

```{r}
#count of values with less than 20000 in the area income column
sum(df_ads$Area.Income<20000)

```

This make 1% of our total data frame dropping the 10 rows would be safer than imputing which may change overall distribution of the data

```{r}
new_df<-df_ads[!(df_ads$Area.Income<20000),]
dim(new_df)
```

**UNIVARIATE ANALYSIS**

*Mean*

```{r}
#this can be used to confirm our calculations on measures if central tendancy

summary(new_df)
```

```{r}
#lets calculate the ,ean /avarages of the numeric variables
#Daily.Time.Spent.on.Site
sum(new_df$Daily.Time.Spent.on.Site)/length(new_df$Daily.Time.Spent.on.Site)
#Age
sum(new_df$Age)/length(new_df$Age)
#Area.Income
sum(new_df$Area.Income)/length(new_df$Area.Income)
#Daily.Internet.Usage
sum(new_df$Daily.Internet.Usage)/length(new_df$Daily.Internet.Usage)


```

*Mode*

Mode is the most recurring number in a given set of numbers

```{r}
#Daily.Time.Spent.on.Site
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
v<-new_df$Daily.Time.Spent.on.Site
result_v <- getmode(v)
print(result_v)
#Age
getmode <- function(p) {
   uniqv <- unique(p)
   uniqv[which.max(tabulate(match(p, uniqv)))]
}
p<-new_df$Age
result_p <- getmode(p)
print(result_p)
#Area.Income
getmode <- function(k) {
   uniqv <- unique(k)
   uniqv[which.max(tabulate(match(k, uniqv)))]
}
k<-new_df$Area.Income
result_k <- getmode(k)
print(result_k)
#Daily.Internet.Usage
getmode <- function(s) {
   uniqv <- unique(s)
   uniqv[which.max(tabulate(match(s, uniqv)))]
}
s<-new_df$Daily.Internet.Usage
result_s <- getmode(s)
print(result_s)

```

*Variance*

Variance is the squared sums of deviations from the mean in a certain variable.

```{r}
#Daily.Time.Spent.on.Site
mn1=sum(new_df$Daily.Time.Spent.on.Site)/length(new_df$Daily.Time.Spent.on.Site)
diff1<-(new_df$Daily.Time.Spent.on.Site-(mn1))
vr1<-sum(diff1^2/new_df$Daily.Time.Spent.on.Site-1)
vr1
#Area.Income
mn2=sum(new_df$Area.Income)/length(new_df$Area.Income)
diff2<-(new_df$Area.Income-(mn2))
vr2<-sum(diff2^2/new_df$Area.Income-1)
vr2
#Age
mn3=sum(new_df$Age)/length(new_df$Agee)
diff3<-(new_df$Age-(mn3))
vr3<-sum(diff3^2/new_df$Age-1)
vr3
#Daily.Internet.Usage
mn4=sum(new_df$Daily.Internet.Usage)/length(new_df$Daily.Internet.Usage)
diff4<-(new_df$Daily.Daily.Internet.Usage-(mn4))
vr4<-sum(diff4^2/new_df$Daily.Internet.Usage-1)
vr4


```

*Standard Deviation*

The standard deviation is a summary measure of the differences of each observation from the mean

```{r}
#Daily.Time.Spent.on.Site SD
sqrt(vr1)
#Age SD
sqrt(vr3)
#Area.Income SD
sqrt(vr2)
#Daily.Internet.UsageSD
sqrt(vr4)

```

*Predictive univariate analysis*

We analyse the properties of single variables and see their contributions towards predicting individuals most likely to click on our ads. Lets start by seeing number of people who clicked the ads.

```{r}
#we change the data type of Clicked.on.Ad to character to syplify our analysis
new_df_Clicked.on.Ad <- transform(new_df, 
                             Clicked.on.Ad = as.character(Clicked.on.Ad))
ads_view<-new_df%>% 
  count(Clicked.on.Ad,sort = TRUE)%>%
  view()
ads_view

```

We see that slightly less than half of the total individuals viewed our ad.Let make a derived data frame from this and explore the individuals that viewed the add\

```{r}
ad_viewers<-new_df[new_df$Clicked.on.Ad==1,]
head(ad_viewers,4)

```

```{r}
Ages=ad_viewers$Age
hist(Ages)

```

The age column distribution is normal most of the ad viewers age are uniformly distributed about the mean on either sides

```{r}
new_df_Male<- transform(new_df, 
                             Male = as.character(Male))
Is_male<-ad_viewers$Male
hist(Is_male)

```

Here we can conclude that most of the ad viewers were female as shown by the higher number of o count which represents not_male

```{r}
ads_view<-new_df%>% 
  count(Age,sort = TRUE)%>%
  view()
head(ads_view,10)
```

Individuals between 26-40 are more frequent on clicking the ads from the frequency table above.

**Bivariate Analysis**

We can now see the relationship between our numerical variables by looking at their covariance coefficients and p-values, scatter plots helps with the visualizations.

*Scatter plots*

```{r}

plot(x = ad_viewers$Age, y = ad_viewers$Daily.Time.Spent.on.Site,
    xlab = "age",
    ylab = "time on site",
          
    main = "age  vs time on site"
)

```

```{r}
#similarly lets calculate their correlation coeficients

cor(ad_viewers$Age, ad_viewers$Daily.Time.Spent.on.Site, method = "pearson")


```

The two variable have weak negative correlation .In this specific data set ,as age increases there is a slight decrease in time spent online,but in this case the decrease is too small almost insignificant.

```{r}
#corelation test
cor.test(ad_viewers$Age, ad_viewers$Daily.Time.Spent.on.Site, method = "pearson")

```

For the two variables the p values is also high meaning that there is little evidence of relationship or difference between the two

```{r}
#correlation matrix
#install.packages("corrplot")
#library("corrplot")
cor(ad_viewers[1:4])

```

```{r}
pairs(ad_viewers[1:4])
```

From the matrix and the pair plots we can see that all of our numerical variables have very little correlation due to their very small correlation coefficients.

**CONCLUTION**

From our finding we can hence conclude :

-   Individuals between 26-40 are more frequent on clicking the ads.

-   Females are more likely to click on our ads.
